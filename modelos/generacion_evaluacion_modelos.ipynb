{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener el mejor modelo, se probaron los 4 modelos disponibles para clasificación en PySpark, en cada uno de estos se realizó el método Grid Search para la optimización de hiperparámetros y se realizó la validación cruzada para garantizar que los resultados de los modelos son independientes de la partición entre datos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos y correción de tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerias necesarias\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/16 19:54:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Generamos sesion de Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RandomForests\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Leemos los datos\n",
    "dataframe = spark.read.parquet(\"malware_data_rev2.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[: bigint, ID: string, size: double, Class: bigint, 0: bigint, 1: bigint, 2: bigint, 3: bigint, 4: bigint, 5: bigint, 6: bigint, 7: bigint, 8: bigint, 9: bigint, 0a: bigint, 0b: bigint, 0c: bigint, 0d: bigint, 0e: bigint, 0f: bigint, 10: bigint, 11: bigint, 12: bigint, 13: bigint, 14: bigint, 15: bigint, 16: bigint, 17: bigint, 18: bigint, 19: bigint, 1a: bigint, 1b: bigint, 1c: bigint, 1d: bigint, 1e: bigint, 1f: bigint, 20: bigint, 21: bigint, 22: bigint, 23: bigint, 24: bigint, 25: bigint, 26: bigint, 27: bigint, 28: bigint, 29: bigint, 2a: bigint, 2b: bigint, 2c: bigint, 2d: bigint, 2e: bigint, 2f: bigint, 30: bigint, 31: bigint, 32: bigint, 33: bigint, 34: bigint, 35: bigint, 36: bigint, 37: bigint, 38: bigint, 39: bigint, 3a: bigint, 3b: bigint, 3c: bigint, 3d: bigint, 3e: bigint, 3f: bigint, 40: bigint, 41: bigint, 42: bigint, 43: bigint, 44: bigint, 45: bigint, 46: bigint, 47: bigint, 48: bigint, 49: bigint, 4a: bigint, 4b: bigint, 4c: bigint, 4d: bigint, 4e: bigint, 4f: bigint, 50: bigint, 51: bigint, 52: bigint, 53: bigint, 54: bigint, 55: bigint, 56: bigint, 57: bigint, 58: bigint, 59: bigint, 5a: bigint, 5b: bigint, 5c: bigint, 5d: bigint, 5e: bigint, 5f: bigint, 60: bigint, 61: bigint, 62: bigint, 63: bigint, 64: bigint, 65: bigint, 66: bigint, 67: bigint, 68: bigint, 69: bigint, 6a: bigint, 6b: bigint, 6c: bigint, 6d: bigint, 6e: bigint, 6f: bigint, 70: bigint, 71: bigint, 72: bigint, 73: bigint, 74: bigint, 75: bigint, 76: bigint, 77: bigint, 78: bigint, 79: bigint, 7a: bigint, 7b: bigint, 7c: bigint, 7d: bigint, 7e: bigint, 7f: bigint, 80: bigint, 81: bigint, 82: bigint, 83: bigint, 84: bigint, 85: bigint, 86: bigint, 87: bigint, 88: bigint, 89: bigint, 8a: bigint, 8b: bigint, 8c: bigint, 8d: bigint, 8e: bigint, 8f: bigint, 90: bigint, 91: bigint, 92: bigint, 93: bigint, 94: bigint, 95: bigint, 96: bigint, 97: bigint, 98: bigint, 99: bigint, 9a: bigint, 9b: bigint, 9c: bigint, 9d: bigint, 9e: bigint, 9f: bigint, a0: bigint, a1: bigint, a2: bigint, a3: bigint, a4: bigint, a5: bigint, a6: bigint, a7: bigint, a8: bigint, a9: bigint, aa: bigint, ab: bigint, ac: bigint, ad: bigint, ae: bigint, af: bigint, b0: bigint, b1: bigint, b2: bigint, b3: bigint, b4: bigint, b5: bigint, b6: bigint, b7: bigint, b8: bigint, b9: bigint, ba: bigint, bb: bigint, bc: bigint, bd: bigint, be: bigint, bf: bigint, c0: bigint, c1: bigint, c2: bigint, c3: bigint, c4: bigint, c5: bigint, c6: bigint, c7: bigint, c8: bigint, c9: bigint, ca: bigint, cb: bigint, cc: bigint, cd: bigint, ce: bigint, cf: bigint, d0: bigint, d1: bigint, d2: bigint, d3: bigint, d4: bigint, d5: bigint, d6: bigint, d7: bigint, d8: bigint, d9: bigint, da: bigint, db: bigint, dc: bigint, dd: bigint, de: bigint, df: bigint, e0: bigint, e1: bigint, e2: bigint, e3: bigint, e4: bigint, e5: bigint, e6: bigint, e7: bigint, e8: bigint, e9: bigint, ea: bigint, eb: bigint, ec: bigint, ed: bigint, ee: bigint, ef: bigint, f0: bigint, f1: bigint, f2: bigint, f3: bigint, f4: bigint, f5: bigint, f6: bigint, f7: bigint, f8: bigint, f9: bigint, fa: bigint, fb: bigint, fc: bigint, fd: bigint, fe: bigint, ff: bigint, ??: bigint]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirmamos que se hayan leido correctamente\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambiamos el formato de los datos\n",
    "\n",
    "\n",
    "dataframe = dataframe.withColumn(\"size\", dataframe[\"size\"].cast(\"float\"))\n",
    "\n",
    "for element in dataframe.columns[3:]:     \n",
    "    dataframe = dataframe.withColumn(element, dataframe[element].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[: bigint, ID: string, size: float, Class: int, 0: int, 1: int, 2: int, 3: int, 4: int, 5: int, 6: int, 7: int, 8: int, 9: int, 0a: int, 0b: int, 0c: int, 0d: int, 0e: int, 0f: int, 10: int, 11: int, 12: int, 13: int, 14: int, 15: int, 16: int, 17: int, 18: int, 19: int, 1a: int, 1b: int, 1c: int, 1d: int, 1e: int, 1f: int, 20: int, 21: int, 22: int, 23: int, 24: int, 25: int, 26: int, 27: int, 28: int, 29: int, 2a: int, 2b: int, 2c: int, 2d: int, 2e: int, 2f: int, 30: int, 31: int, 32: int, 33: int, 34: int, 35: int, 36: int, 37: int, 38: int, 39: int, 3a: int, 3b: int, 3c: int, 3d: int, 3e: int, 3f: int, 40: int, 41: int, 42: int, 43: int, 44: int, 45: int, 46: int, 47: int, 48: int, 49: int, 4a: int, 4b: int, 4c: int, 4d: int, 4e: int, 4f: int, 50: int, 51: int, 52: int, 53: int, 54: int, 55: int, 56: int, 57: int, 58: int, 59: int, 5a: int, 5b: int, 5c: int, 5d: int, 5e: int, 5f: int, 60: int, 61: int, 62: int, 63: int, 64: int, 65: int, 66: int, 67: int, 68: int, 69: int, 6a: int, 6b: int, 6c: int, 6d: int, 6e: int, 6f: int, 70: int, 71: int, 72: int, 73: int, 74: int, 75: int, 76: int, 77: int, 78: int, 79: int, 7a: int, 7b: int, 7c: int, 7d: int, 7e: int, 7f: int, 80: int, 81: int, 82: int, 83: int, 84: int, 85: int, 86: int, 87: int, 88: int, 89: int, 8a: int, 8b: int, 8c: int, 8d: int, 8e: int, 8f: int, 90: int, 91: int, 92: int, 93: int, 94: int, 95: int, 96: int, 97: int, 98: int, 99: int, 9a: int, 9b: int, 9c: int, 9d: int, 9e: int, 9f: int, a0: int, a1: int, a2: int, a3: int, a4: int, a5: int, a6: int, a7: int, a8: int, a9: int, aa: int, ab: int, ac: int, ad: int, ae: int, af: int, b0: int, b1: int, b2: int, b3: int, b4: int, b5: int, b6: int, b7: int, b8: int, b9: int, ba: int, bb: int, bc: int, bd: int, be: int, bf: int, c0: int, c1: int, c2: int, c3: int, c4: int, c5: int, c6: int, c7: int, c8: int, c9: int, ca: int, cb: int, cc: int, cd: int, ce: int, cf: int, d0: int, d1: int, d2: int, d3: int, d4: int, d5: int, d6: int, d7: int, d8: int, d9: int, da: int, db: int, dc: int, dd: int, de: int, df: int, e0: int, e1: int, e2: int, e3: int, e4: int, e5: int, e6: int, e7: int, e8: int, e9: int, ea: int, eb: int, ec: int, ed: int, ee: int, ef: int, f0: int, f1: int, f2: int, f3: int, f4: int, f5: int, f6: int, f7: int, f8: int, f9: int, fa: int, fb: int, fc: int, fd: int, fe: int, ff: int, ??: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirmamos el cambio de datos\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:54:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|   | ID|size|Class|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9| 0a| 0b| 0c| 0d| 0e| 0f| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19| 1a| 1b| 1c| 1d| 1e| 1f| 20| 21| 22| 23| 24| 25| 26| 27| 28| 29| 2a| 2b| 2c| 2d| 2e| 2f| 30| 31| 32| 33| 34| 35| 36| 37| 38| 39| 3a| 3b| 3c| 3d| 3e| 3f| 40| 41| 42| 43| 44| 45| 46| 47| 48| 49| 4a| 4b| 4c| 4d| 4e| 4f| 50| 51| 52| 53| 54| 55| 56| 57| 58| 59| 5a| 5b| 5c| 5d| 5e| 5f| 60| 61| 62| 63| 64| 65| 66| 67| 68| 69| 6a| 6b| 6c| 6d| 6e| 6f| 70| 71| 72| 73| 74| 75| 76| 77| 78| 79| 7a| 7b| 7c| 7d| 7e| 7f| 80| 81| 82| 83| 84| 85| 86| 87| 88| 89| 8a| 8b| 8c| 8d| 8e| 8f| 90| 91| 92| 93| 94| 95| 96| 97| 98| 99| 9a| 9b| 9c| 9d| 9e| 9f| a0| a1| a2| a3| a4| a5| a6| a7| a8| a9| aa| ab| ac| ad| ae| af| b0| b1| b2| b3| b4| b5| b6| b7| b8| b9| ba| bb| bc| bd| be| bf| c0| c1| c2| c3| c4| c5| c6| c7| c8| c9| ca| cb| cc| cd| ce| cf| d0| d1| d2| d3| d4| d5| d6| d7| d8| d9| da| db| dc| dd| de| df| e0| e1| e2| e3| e4| e5| e6| e7| e8| e9| ea| eb| ec| ed| ee| ef| f0| f1| f2| f3| f4| f5| f6| f7| f8| f9| fa| fb| fc| fd| fe| ff| ??|\n",
      "+---+---+----+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|  0|  0|   0|    0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+---+---+----+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Observamos el numero de valores perdidos\n",
    "dataframe.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dataframe.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removemos la primera columna\n",
    "dataframe = dataframe.drop('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de datos y train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+--------------------+\n",
      "|                  ID|    size|Class|    0|   1|   2|   3|   4|   5|   6|   7|   8|   9|  0a|  0b|  0c|  0d|  0e|  0f|  10|  11|  12|  13|  14|  15|  16|  17|  18|  19|  1a|  1b|  1c|  1d|  1e|  1f|  20|  21|  22|  23|  24|  25|  26|  27|  28|  29|  2a|  2b|  2c|  2d|  2e|  2f|  30|  31|  32|  33|  34|  35|  36|  37|  38|  39|  3a|  3b|  3c|  3d|  3e|  3f|  40|  41|  42|  43|  44|  45|  46|  47|  48|  49|  4a|  4b|  4c|  4d|  4e|  4f|  50|  51|  52|  53|  54|  55|  56|  57|  58|  59|  5a|  5b|  5c|  5d|  5e|  5f|  60|  61|  62|  63|  64|  65|  66|  67|  68|  69|  6a|  6b|  6c|  6d|  6e|  6f|  70|  71|  72|  73|  74|  75|  76|  77|  78|  79|  7a|  7b|  7c|  7d|  7e|  7f|  80|  81|  82|  83|  84|  85|  86|  87|  88|  89|  8a|  8b|  8c|  8d|  8e|  8f|  90|  91|  92|  93|  94|  95|  96|  97|  98|  99|  9a|  9b|  9c|  9d|  9e|  9f|  a0|  a1|  a2|  a3|  a4|  a5|  a6|  a7|  a8|  a9|  aa|  ab|  ac|  ad|  ae|  af|  b0|  b1|  b2|  b3|  b4|  b5|  b6|  b7|  b8|  b9|  ba|  bb|  bc|  bd|  be|  bf|  c0|  c1|  c2|  c3|  c4|  c5|  c6|  c7|  c8|  c9|  ca|  cb|  cc|  cd|  ce|  cf|  d0|  d1|  d2|  d3|  d4|  d5|  d6|  d7|  d8|  d9|  da|  db|  dc|  dd|  de|  df|  e0|  e1|  e2|  e3|  e4|  e5|  e6|  e7|  e8|  e9|  ea|  eb|  ec|  ed|  ee|  ef|  f0|  f1|  f2|  f3|  f4|  f5|  f6|  f7|  f8|  f9|  fa|  fb|  fc|  fd|  fe|  ff|     ??|            features|\n",
      "+--------------------+--------+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+--------------------+\n",
      "|04BfoQRA6XEshiNuI7pF|8.941406|    3|14119|8409|6122|6122|6228|6228|6349|6279|6112|6174|6371|6295|6184|6399|6347|6603|6312|6083|6396|6240|6180|6474|6125|6213|6278|6179|6187|6054|6408|6024|6176|6140|6223|6447|6023|6221|6125|6296|6100|6136|6054|6129|6125|6038|6055|6328|6302|6016|6139|6167|6328|6194|6157|6157|6222|6148|5848|6186|6166|6178|6199|6075|6301|6139|6196|6187|6396|6161|6174|6156|6202|6123|6238|6050|6270|6190|6231|6065|6438|6152|6266|6322|6235|6031|6347|6427|6128|6320|6031|6402|6410|6120|6405|6203|6087|6231|6320|6241|6123|6436|6302|6211|6172|5993|6170|6078|6397|6114|6209|6224|6129|6125|6272|6340|6367|6209|6184|6158|6322|6261|6024|6062|6161|6205|6167|6098|6171|6197|6610|6189|6228|6413|6083|6296|6256|6208|6400|6208|6268|6500|6169|6526|6045|6249|6143|6157|6218|6217|6424|6190|6358|6615|6213|6136|6234|6270|6161|6074|6000|6138|6101|6130|6103|6310|6145|6180|6447|5994|6224|6098|6180|6158|6303|6322|6133|6137|6262|6267|6177|6183|5976|6378|6252|6455|6103|6283|6413|6172|6294|6212|6134|6203|6298|6114|6125|6370|6184|6254|5991|6262|6284|6118|6114|6092|6384|6284|5858|6074|6224|6034|6172|5916|6180|6168|6394|6276|6086|6150|6224|6123|5860|6268|6072|6054|6074|6154|6096|6121|6151|6134|5968|6053|6365|6170|5990|6181|6265|6126|6028|6309|8169|8608|8311|6146|6070|6196|6337|6048|6163|6255|6169|5936|6240|6034|6228|9829|1517724|[14119.0,8409.0,6...|\n",
      "+--------------------+--------+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Generamos una nueva columna que tenga todas las caracteristicas que usaremos(asi lo piden los modelos de pyspark)\n",
    "\n",
    "numericCols = list(dataframe.columns[3:])\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "dataframe_rf = assembler.transform(dataframe)\n",
    "dataframe_rf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 7654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Count: 3214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Train/test split\n",
    "train, test = dataframe_rf.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomforests\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\")\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'Class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Param grid del random forests\n",
    "rfparamGrid = (ParamGridBuilder()\n",
    "               .addGrid(rf.maxDepth, [2, 5, 10])\n",
    "               .addGrid(rf.maxBins, [5, 10, 20])\n",
    "               .addGrid(rf.numTrees, [5, 20, 50])\n",
    "             .build())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcv = CrossValidator(estimator = rf,\n",
    "                      estimatorParamMaps = rfparamGrid,\n",
    "                      evaluator = evaluator,\n",
    "                      numFolds = 3,\n",
    "                      parallelism=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:55:57 WARN DAGScheduler: Broadcasting large task binary with size 1197.9 KiB\n",
      "22/05/16 19:55:58 WARN DAGScheduler: Broadcasting large task binary with size 1472.8 KiB\n",
      "22/05/16 19:56:06 WARN DAGScheduler: Broadcasting large task binary with size 1150.9 KiB\n",
      "22/05/16 19:56:12 WARN DAGScheduler: Broadcasting large task binary with size 1405.7 KiB\n",
      "22/05/16 19:56:13 WARN DAGScheduler: Broadcasting large task binary with size 1063.3 KiB\n",
      "22/05/16 19:56:14 WARN DAGScheduler: Broadcasting large task binary with size 1300.1 KiB\n",
      "22/05/16 19:56:14 WARN DAGScheduler: Broadcasting large task binary with size 2029.0 KiB\n",
      "22/05/16 19:56:14 WARN DAGScheduler: Broadcasting large task binary with size 1265.5 KiB\n",
      "22/05/16 19:56:16 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/05/16 19:56:16 WARN DAGScheduler: Broadcasting large task binary with size 1744.7 KiB\n",
      "22/05/16 19:56:16 WARN DAGScheduler: Broadcasting large task binary with size 1037.9 KiB\n",
      "22/05/16 19:56:17 WARN DAGScheduler: Broadcasting large task binary with size 1264.0 KiB\n",
      "22/05/16 19:56:17 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "22/05/16 19:56:18 WARN DAGScheduler: Broadcasting large task binary with size 1284.8 KiB\n",
      "22/05/16 19:56:18 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/05/16 19:56:19 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/05/16 19:56:19 WARN DAGScheduler: Broadcasting large task binary with size 1762.3 KiB\n",
      "22/05/16 19:56:20 WARN DAGScheduler: Broadcasting large task binary with size 1034.3 KiB\n",
      "22/05/16 19:56:21 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/05/16 19:56:23 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "22/05/16 19:56:23 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/05/16 19:56:25 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/05/16 19:56:28 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/05/16 19:57:10 WARN DAGScheduler: Broadcasting large task binary with size 1240.7 KiB\n",
      "22/05/16 19:57:10 WARN DAGScheduler: Broadcasting large task binary with size 1520.9 KiB\n",
      "22/05/16 19:57:18 WARN DAGScheduler: Broadcasting large task binary with size 1178.8 KiB\n",
      "22/05/16 19:57:24 WARN DAGScheduler: Broadcasting large task binary with size 1422.9 KiB\n",
      "22/05/16 19:57:24 WARN DAGScheduler: Broadcasting large task binary with size 1048.8 KiB\n",
      "22/05/16 19:57:25 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/05/16 19:57:25 WARN DAGScheduler: Broadcasting large task binary with size 1290.2 KiB\n",
      "22/05/16 19:57:26 WARN DAGScheduler: Broadcasting large task binary with size 1264.4 KiB\n",
      "22/05/16 19:57:26 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/05/16 19:57:27 WARN DAGScheduler: Broadcasting large task binary with size 1055.8 KiB\n",
      "22/05/16 19:57:28 WARN DAGScheduler: Broadcasting large task binary with size 1760.8 KiB\n",
      "22/05/16 19:57:28 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "22/05/16 19:57:29 WARN DAGScheduler: Broadcasting large task binary with size 1284.8 KiB\n",
      "22/05/16 19:57:29 WARN DAGScheduler: Broadcasting large task binary with size 1256.5 KiB\n",
      "22/05/16 19:57:29 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/05/16 19:57:30 WARN DAGScheduler: Broadcasting large task binary with size 1029.6 KiB\n",
      "22/05/16 19:57:31 WARN DAGScheduler: Broadcasting large task binary with size 1731.2 KiB\n",
      "22/05/16 19:57:31 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "22/05/16 19:57:32 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/05/16 19:57:33 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "22/05/16 19:57:33 WARN DAGScheduler: Broadcasting large task binary with size 1005.7 KiB\n",
      "22/05/16 19:57:34 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/05/16 19:57:36 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/05/16 19:57:39 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/05/16 19:58:21 WARN DAGScheduler: Broadcasting large task binary with size 1200.8 KiB\n",
      "22/05/16 19:58:21 WARN DAGScheduler: Broadcasting large task binary with size 1484.8 KiB\n",
      "22/05/16 19:58:29 WARN DAGScheduler: Broadcasting large task binary with size 1160.8 KiB\n",
      "22/05/16 19:58:36 WARN DAGScheduler: Broadcasting large task binary with size 1417.4 KiB\n",
      "22/05/16 19:58:36 WARN DAGScheduler: Broadcasting large task binary with size 1067.0 KiB\n",
      "22/05/16 19:58:37 WARN DAGScheduler: Broadcasting large task binary with size 1313.0 KiB\n",
      "22/05/16 19:58:38 WARN DAGScheduler: Broadcasting large task binary with size 2031.2 KiB\n",
      "22/05/16 19:58:38 WARN DAGScheduler: Broadcasting large task binary with size 1251.2 KiB\n",
      "22/05/16 19:58:39 WARN DAGScheduler: Broadcasting large task binary with size 1061.4 KiB\n",
      "22/05/16 19:58:39 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "22/05/16 19:58:40 WARN DAGScheduler: Broadcasting large task binary with size 1718.8 KiB\n",
      "22/05/16 19:58:40 WARN DAGScheduler: Broadcasting large task binary with size 1296.0 KiB\n",
      "22/05/16 19:58:40 WARN DAGScheduler: Broadcasting large task binary with size 1252.0 KiB\n",
      "22/05/16 19:58:41 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "22/05/16 19:58:41 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/05/16 19:58:41 WARN DAGScheduler: Broadcasting large task binary with size 1704.2 KiB\n",
      "22/05/16 19:58:43 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/05/16 19:58:43 WARN DAGScheduler: Broadcasting large task binary with size 1050.1 KiB\n",
      "22/05/16 19:58:43 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/05/16 19:58:45 WARN DAGScheduler: Broadcasting large task binary with size 1010.7 KiB\n",
      "22/05/16 19:58:46 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/05/16 19:58:46 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "22/05/16 19:58:48 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/05/16 19:58:50 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/05/16 19:59:09 WARN DAGScheduler: Broadcasting large task binary with size 1328.4 KiB\n",
      "22/05/16 19:59:10 WARN DAGScheduler: Broadcasting large task binary with size 1880.2 KiB\n",
      "22/05/16 19:59:12 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "22/05/16 19:59:14 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidatorModel_6963904a0c29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:59:21 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "[Stage 1685:============================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9741284343866718\n",
      "Test Error = 0.02587156561332815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rfcvModel = rfcv.fit(train)\n",
    "print(rfcvModel)\n",
    "\n",
    "predictions = rfcvModel.transform(test)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel: uid=RandomForestClassifier_207c18c9c6b0, numTrees=50, numClasses=10, numFeatures=257"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Los parametros que dieron el mejor accuracy\n",
    "rfcvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"Class\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtparamGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [2, 5, 10, 20, 30])\n",
    "             .addGrid(dt.maxBins, [10, 20, 40, 80, 100])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtevaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcv = CrossValidator(estimator = dt,\n",
    "                      estimatorParamMaps = dtparamGrid,\n",
    "                      evaluator = dtevaluator,\n",
    "                      numFolds = 3,\n",
    "                      parallelism = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 20:03:09 WARN BlockManager: Asked to remove block broadcast_5145, which does not exist\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dtcvModel = dtcv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4177:============================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9601742377100186\n",
      "Test Error = 0.03982576228998136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = dtcvModel.transform(test)\n",
    "\n",
    "accuracy = dtevaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_79dc8af94a40, depth=20, numNodes=447, numClasses=10, numFeatures=257"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtcvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 20:04:01 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/05/16 20:04:01 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/05/16 20:04:02 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/05/16 20:04:02 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "#Regresión logística con parámetros default\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'Class')\n",
    "lrModel = lr.fit(train)\n",
    "prediction = lrModel.transform(test)\n",
    "accuracy = prediction.filter(prediction.Class == prediction.prediction).count() / float(prediction.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4292:============================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9172370877411326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prediction = lrModel.transform(test)\n",
    "accuracy = prediction.filter(prediction.Class == prediction.prediction).count() / float(prediction.count())\n",
    "\n",
    "print(\"Accuracy : \",accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizamos ParamGrid para establecer diferentes valores para los parámetros y un Evaluator para usar Cross Validation\n",
    "\n",
    "lrparamGrid = (ParamGridBuilder()\n",
    "               .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "               .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "               .addGrid(lr.maxIter, [10, 50,100])\n",
    "               .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv = CrossValidator(estimator = lr,\n",
    "                      estimatorParamMaps = lrparamGrid,\n",
    "                      evaluator = evaluator,\n",
    "                      numFolds = 5,\n",
    "                      parallelism = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 20:06:27 WARN BlockManager: Asked to remove block broadcast_12136_piece0, which does not exist\n",
      "22/05/16 20:06:27 WARN BlockManager: Asked to remove block broadcast_12136, which does not exist\n",
      "22/05/16 20:08:56 WARN BlockManager: Asked to remove block broadcast_19647, which does not exist\n",
      "22/05/16 20:09:16 WARN BlockManager: Asked to remove block broadcast_21074, which does not exist\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Entrenamos el modelo con los datos de entrenamiento\n",
    "lrcvModel = lrcv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12746:===========================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7767209222217072\n",
      "Test Error = 0.2232790777782928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Predecimos con datos de test y calculamos el accuracy\n",
    "predictions = lrcvModel.transform(test)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param (regParam):  0.01\n",
      "Best Param (MaxIter):  50\n",
      "Best Param (elasticNetParam):  0.0\n"
     ]
    }
   ],
   "source": [
    "#Calculamos los parámetros que nos resultan en el mejor modelo \n",
    "best_lr= lrcvModel.bestModel\n",
    "\n",
    "print('Best Param (regParam): ',best_lr._java_obj.getRegParam())\n",
    "print('Best Param (MaxIter): ',best_lr._java_obj.getMaxIter())\n",
    "print('Best Param (elasticNetParam): ',best_lr._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(labelCol=\"Class\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbparamGrid = (ParamGridBuilder()\n",
    "               .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "               .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbevaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbcv = CrossValidator(estimator = nb,\n",
    "                      estimatorParamMaps = nbparamGrid,\n",
    "                      evaluator = nbevaluator,\n",
    "                      numFolds = 5,\n",
    "                     parallelism=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nbcvModel = nbcv.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbpredictions = nbcvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12901:===========================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.962352 \n",
      "Accuracy = 0.03764779091474798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "accuracy = nbevaluator.evaluate(nbpredictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "print(\"Accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesModel: uid=NaiveBayes_5cb1a730bcc0, modelType=multinomial, numClasses=9, numFeatures=257\n"
     ]
    }
   ],
   "source": [
    "best_nb =nbcvModel.bestModel\n",
    "\n",
    "print(best_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando el mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportando el modelo\n",
    "dtcvModel.save('best_model')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
