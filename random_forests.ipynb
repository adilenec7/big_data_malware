{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests en pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/10 09:21:23 WARN Utils: Your hostname, mcd resolves to a loopback address: 127.0.1.1; using 192.168.1.81 instead (on interface wlp0s20f3)\n",
      "22/05/10 09:21:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/carlosv/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/10 09:21:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Generamos sesion de Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RandomForests\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Leemos los datos\n",
    "dataframe = spark.read.parquet(\"malware_data_rev2.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[: bigint, ID: string, size: double, Class: bigint, 0: bigint, 1: bigint, 2: bigint, 3: bigint, 4: bigint, 5: bigint, 6: bigint, 7: bigint, 8: bigint, 9: bigint, 0a: bigint, 0b: bigint, 0c: bigint, 0d: bigint, 0e: bigint, 0f: bigint, 10: bigint, 11: bigint, 12: bigint, 13: bigint, 14: bigint, 15: bigint, 16: bigint, 17: bigint, 18: bigint, 19: bigint, 1a: bigint, 1b: bigint, 1c: bigint, 1d: bigint, 1e: bigint, 1f: bigint, 20: bigint, 21: bigint, 22: bigint, 23: bigint, 24: bigint, 25: bigint, 26: bigint, 27: bigint, 28: bigint, 29: bigint, 2a: bigint, 2b: bigint, 2c: bigint, 2d: bigint, 2e: bigint, 2f: bigint, 30: bigint, 31: bigint, 32: bigint, 33: bigint, 34: bigint, 35: bigint, 36: bigint, 37: bigint, 38: bigint, 39: bigint, 3a: bigint, 3b: bigint, 3c: bigint, 3d: bigint, 3e: bigint, 3f: bigint, 40: bigint, 41: bigint, 42: bigint, 43: bigint, 44: bigint, 45: bigint, 46: bigint, 47: bigint, 48: bigint, 49: bigint, 4a: bigint, 4b: bigint, 4c: bigint, 4d: bigint, 4e: bigint, 4f: bigint, 50: bigint, 51: bigint, 52: bigint, 53: bigint, 54: bigint, 55: bigint, 56: bigint, 57: bigint, 58: bigint, 59: bigint, 5a: bigint, 5b: bigint, 5c: bigint, 5d: bigint, 5e: bigint, 5f: bigint, 60: bigint, 61: bigint, 62: bigint, 63: bigint, 64: bigint, 65: bigint, 66: bigint, 67: bigint, 68: bigint, 69: bigint, 6a: bigint, 6b: bigint, 6c: bigint, 6d: bigint, 6e: bigint, 6f: bigint, 70: bigint, 71: bigint, 72: bigint, 73: bigint, 74: bigint, 75: bigint, 76: bigint, 77: bigint, 78: bigint, 79: bigint, 7a: bigint, 7b: bigint, 7c: bigint, 7d: bigint, 7e: bigint, 7f: bigint, 80: bigint, 81: bigint, 82: bigint, 83: bigint, 84: bigint, 85: bigint, 86: bigint, 87: bigint, 88: bigint, 89: bigint, 8a: bigint, 8b: bigint, 8c: bigint, 8d: bigint, 8e: bigint, 8f: bigint, 90: bigint, 91: bigint, 92: bigint, 93: bigint, 94: bigint, 95: bigint, 96: bigint, 97: bigint, 98: bigint, 99: bigint, 9a: bigint, 9b: bigint, 9c: bigint, 9d: bigint, 9e: bigint, 9f: bigint, a0: bigint, a1: bigint, a2: bigint, a3: bigint, a4: bigint, a5: bigint, a6: bigint, a7: bigint, a8: bigint, a9: bigint, aa: bigint, ab: bigint, ac: bigint, ad: bigint, ae: bigint, af: bigint, b0: bigint, b1: bigint, b2: bigint, b3: bigint, b4: bigint, b5: bigint, b6: bigint, b7: bigint, b8: bigint, b9: bigint, ba: bigint, bb: bigint, bc: bigint, bd: bigint, be: bigint, bf: bigint, c0: bigint, c1: bigint, c2: bigint, c3: bigint, c4: bigint, c5: bigint, c6: bigint, c7: bigint, c8: bigint, c9: bigint, ca: bigint, cb: bigint, cc: bigint, cd: bigint, ce: bigint, cf: bigint, d0: bigint, d1: bigint, d2: bigint, d3: bigint, d4: bigint, d5: bigint, d6: bigint, d7: bigint, d8: bigint, d9: bigint, da: bigint, db: bigint, dc: bigint, dd: bigint, de: bigint, df: bigint, e0: bigint, e1: bigint, e2: bigint, e3: bigint, e4: bigint, e5: bigint, e6: bigint, e7: bigint, e8: bigint, e9: bigint, ea: bigint, eb: bigint, ec: bigint, ed: bigint, ee: bigint, ef: bigint, f0: bigint, f1: bigint, f2: bigint, f3: bigint, f4: bigint, f5: bigint, f6: bigint, f7: bigint, f8: bigint, f9: bigint, fa: bigint, fb: bigint, fc: bigint, fd: bigint, fe: bigint, ff: bigint, ??: bigint]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirmamos que se hayan leido correctamente\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambiamos el formato de los datos\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "dataframe = dataframe.withColumn(\"size\", dataframe[\"size\"].cast(\"float\"))\n",
    "\n",
    "for element in dataframe.columns[2:]:     \n",
    "    dataframe = dataframe.withColumn(element, dataframe[element].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[: bigint, ID: string, size: int, Class: int, 0: int, 1: int, 2: int, 3: int, 4: int, 5: int, 6: int, 7: int, 8: int, 9: int, 0a: int, 0b: int, 0c: int, 0d: int, 0e: int, 0f: int, 10: int, 11: int, 12: int, 13: int, 14: int, 15: int, 16: int, 17: int, 18: int, 19: int, 1a: int, 1b: int, 1c: int, 1d: int, 1e: int, 1f: int, 20: int, 21: int, 22: int, 23: int, 24: int, 25: int, 26: int, 27: int, 28: int, 29: int, 2a: int, 2b: int, 2c: int, 2d: int, 2e: int, 2f: int, 30: int, 31: int, 32: int, 33: int, 34: int, 35: int, 36: int, 37: int, 38: int, 39: int, 3a: int, 3b: int, 3c: int, 3d: int, 3e: int, 3f: int, 40: int, 41: int, 42: int, 43: int, 44: int, 45: int, 46: int, 47: int, 48: int, 49: int, 4a: int, 4b: int, 4c: int, 4d: int, 4e: int, 4f: int, 50: int, 51: int, 52: int, 53: int, 54: int, 55: int, 56: int, 57: int, 58: int, 59: int, 5a: int, 5b: int, 5c: int, 5d: int, 5e: int, 5f: int, 60: int, 61: int, 62: int, 63: int, 64: int, 65: int, 66: int, 67: int, 68: int, 69: int, 6a: int, 6b: int, 6c: int, 6d: int, 6e: int, 6f: int, 70: int, 71: int, 72: int, 73: int, 74: int, 75: int, 76: int, 77: int, 78: int, 79: int, 7a: int, 7b: int, 7c: int, 7d: int, 7e: int, 7f: int, 80: int, 81: int, 82: int, 83: int, 84: int, 85: int, 86: int, 87: int, 88: int, 89: int, 8a: int, 8b: int, 8c: int, 8d: int, 8e: int, 8f: int, 90: int, 91: int, 92: int, 93: int, 94: int, 95: int, 96: int, 97: int, 98: int, 99: int, 9a: int, 9b: int, 9c: int, 9d: int, 9e: int, 9f: int, a0: int, a1: int, a2: int, a3: int, a4: int, a5: int, a6: int, a7: int, a8: int, a9: int, aa: int, ab: int, ac: int, ad: int, ae: int, af: int, b0: int, b1: int, b2: int, b3: int, b4: int, b5: int, b6: int, b7: int, b8: int, b9: int, ba: int, bb: int, bc: int, bd: int, be: int, bf: int, c0: int, c1: int, c2: int, c3: int, c4: int, c5: int, c6: int, c7: int, c8: int, c9: int, ca: int, cb: int, cc: int, cd: int, ce: int, cf: int, d0: int, d1: int, d2: int, d3: int, d4: int, d5: int, d6: int, d7: int, d8: int, d9: int, da: int, db: int, dc: int, dd: int, de: int, df: int, e0: int, e1: int, e2: int, e3: int, e4: int, e5: int, e6: int, e7: int, e8: int, e9: int, ea: int, eb: int, ec: int, ed: int, ee: int, ef: int, f0: int, f1: int, f2: int, f3: int, f4: int, f5: int, f6: int, f7: int, f8: int, f9: int, fa: int, fb: int, fc: int, fd: int, fe: int, ff: int, ??: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirmamos el cambio de datos\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/10 09:22:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|   | ID|size|Class|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9| 0a| 0b| 0c| 0d| 0e| 0f| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19| 1a| 1b| 1c| 1d| 1e| 1f| 20| 21| 22| 23| 24| 25| 26| 27| 28| 29| 2a| 2b| 2c| 2d| 2e| 2f| 30| 31| 32| 33| 34| 35| 36| 37| 38| 39| 3a| 3b| 3c| 3d| 3e| 3f| 40| 41| 42| 43| 44| 45| 46| 47| 48| 49| 4a| 4b| 4c| 4d| 4e| 4f| 50| 51| 52| 53| 54| 55| 56| 57| 58| 59| 5a| 5b| 5c| 5d| 5e| 5f| 60| 61| 62| 63| 64| 65| 66| 67| 68| 69| 6a| 6b| 6c| 6d| 6e| 6f| 70| 71| 72| 73| 74| 75| 76| 77| 78| 79| 7a| 7b| 7c| 7d| 7e| 7f| 80| 81| 82| 83| 84| 85| 86| 87| 88| 89| 8a| 8b| 8c| 8d| 8e| 8f| 90| 91| 92| 93| 94| 95| 96| 97| 98| 99| 9a| 9b| 9c| 9d| 9e| 9f| a0| a1| a2| a3| a4| a5| a6| a7| a8| a9| aa| ab| ac| ad| ae| af| b0| b1| b2| b3| b4| b5| b6| b7| b8| b9| ba| bb| bc| bd| be| bf| c0| c1| c2| c3| c4| c5| c6| c7| c8| c9| ca| cb| cc| cd| ce| cf| d0| d1| d2| d3| d4| d5| d6| d7| d8| d9| da| db| dc| dd| de| df| e0| e1| e2| e3| e4| e5| e6| e7| e8| e9| ea| eb| ec| ed| ee| ef| f0| f1| f2| f3| f4| f5| f6| f7| f8| f9| fa| fb| fc| fd| fe| ff| ??|\n",
      "+---+---+----+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|  0|  0|   0|    0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+---+---+----+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Observamos el numero de valores perdidos\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "dataframe.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dataframe.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removemos la primera columna\n",
    "dataframe = dataframe.drop('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+--------------------+\n",
      "|                  ID|size|Class|    0|   1|   2|   3|   4|   5|   6|   7|   8|   9|  0a|  0b|  0c|  0d|  0e|  0f|  10|  11|  12|  13|  14|  15|  16|  17|  18|  19|  1a|  1b|  1c|  1d|  1e|  1f|  20|  21|  22|  23|  24|  25|  26|  27|  28|  29|  2a|  2b|  2c|  2d|  2e|  2f|  30|  31|  32|  33|  34|  35|  36|  37|  38|  39|  3a|  3b|  3c|  3d|  3e|  3f|  40|  41|  42|  43|  44|  45|  46|  47|  48|  49|  4a|  4b|  4c|  4d|  4e|  4f|  50|  51|  52|  53|  54|  55|  56|  57|  58|  59|  5a|  5b|  5c|  5d|  5e|  5f|  60|  61|  62|  63|  64|  65|  66|  67|  68|  69|  6a|  6b|  6c|  6d|  6e|  6f|  70|  71|  72|  73|  74|  75|  76|  77|  78|  79|  7a|  7b|  7c|  7d|  7e|  7f|  80|  81|  82|  83|  84|  85|  86|  87|  88|  89|  8a|  8b|  8c|  8d|  8e|  8f|  90|  91|  92|  93|  94|  95|  96|  97|  98|  99|  9a|  9b|  9c|  9d|  9e|  9f|  a0|  a1|  a2|  a3|  a4|  a5|  a6|  a7|  a8|  a9|  aa|  ab|  ac|  ad|  ae|  af|  b0|  b1|  b2|  b3|  b4|  b5|  b6|  b7|  b8|  b9|  ba|  bb|  bc|  bd|  be|  bf|  c0|  c1|  c2|  c3|  c4|  c5|  c6|  c7|  c8|  c9|  ca|  cb|  cc|  cd|  ce|  cf|  d0|  d1|  d2|  d3|  d4|  d5|  d6|  d7|  d8|  d9|  da|  db|  dc|  dd|  de|  df|  e0|  e1|  e2|  e3|  e4|  e5|  e6|  e7|  e8|  e9|  ea|  eb|  ec|  ed|  ee|  ef|  f0|  f1|  f2|  f3|  f4|  f5|  f6|  f7|  f8|  f9|  fa|  fb|  fc|  fd|  fe|  ff|     ??|            features|\n",
      "+--------------------+----+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+--------------------+\n",
      "|04BfoQRA6XEshiNuI7pF|   8|    3|14119|8409|6122|6122|6228|6228|6349|6279|6112|6174|6371|6295|6184|6399|6347|6603|6312|6083|6396|6240|6180|6474|6125|6213|6278|6179|6187|6054|6408|6024|6176|6140|6223|6447|6023|6221|6125|6296|6100|6136|6054|6129|6125|6038|6055|6328|6302|6016|6139|6167|6328|6194|6157|6157|6222|6148|5848|6186|6166|6178|6199|6075|6301|6139|6196|6187|6396|6161|6174|6156|6202|6123|6238|6050|6270|6190|6231|6065|6438|6152|6266|6322|6235|6031|6347|6427|6128|6320|6031|6402|6410|6120|6405|6203|6087|6231|6320|6241|6123|6436|6302|6211|6172|5993|6170|6078|6397|6114|6209|6224|6129|6125|6272|6340|6367|6209|6184|6158|6322|6261|6024|6062|6161|6205|6167|6098|6171|6197|6610|6189|6228|6413|6083|6296|6256|6208|6400|6208|6268|6500|6169|6526|6045|6249|6143|6157|6218|6217|6424|6190|6358|6615|6213|6136|6234|6270|6161|6074|6000|6138|6101|6130|6103|6310|6145|6180|6447|5994|6224|6098|6180|6158|6303|6322|6133|6137|6262|6267|6177|6183|5976|6378|6252|6455|6103|6283|6413|6172|6294|6212|6134|6203|6298|6114|6125|6370|6184|6254|5991|6262|6284|6118|6114|6092|6384|6284|5858|6074|6224|6034|6172|5916|6180|6168|6394|6276|6086|6150|6224|6123|5860|6268|6072|6054|6074|6154|6096|6121|6151|6134|5968|6053|6365|6170|5990|6181|6265|6126|6028|6309|8169|8608|8311|6146|6070|6196|6337|6048|6163|6255|6169|5936|6240|6034|6228|9829|1517724|[14119.0,8409.0,6...|\n",
      "+--------------------+----+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Generamos una nueva columna que tenga todas las caracteristicas que usaremos(asi lo piden los modelos de pyspark)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numericCols = list(dataframe.columns[3:])\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "dataframe_rf = assembler.transform(dataframe)\n",
    "dataframe_rf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 7654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Count: 3214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Train/test split\n",
    "train, test = dataframe_rf.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomforests\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\")\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'Class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Param grid del random forests\n",
    "rfparamGrid = (ParamGridBuilder()\n",
    "             #.addGrid(rf.maxDepth, [2, 5, 10, 20, 30])\n",
    "               .addGrid(rf.maxDepth, [2, 5, 10])\n",
    "             #.addGrid(rf.maxBins, [10, 20, 40, 80, 100])\n",
    "               .addGrid(rf.maxBins, [5, 10, 20])\n",
    "             #.addGrid(rf.numTrees, [5, 20, 50, 100, 500])\n",
    "               .addGrid(rf.numTrees, [5, 20, 50])\n",
    "             .build())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcv = CrossValidator(estimator = rf,\n",
    "                      estimatorParamMaps = rfparamGrid,\n",
    "                      evaluator = evaluator,\n",
    "                      numFolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Class|prediction|\n",
      "+-----+----------+\n",
      "|    2|       2.0|\n",
      "|    2|       2.0|\n",
      "|    6|       6.0|\n",
      "|    3|       3.0|\n",
      "|    8|       8.0|\n",
      "|    1|       1.0|\n",
      "|    2|       2.0|\n",
      "|    3|       3.0|\n",
      "|    2|       2.0|\n",
      "|    3|       3.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Algunas de las predicciones\n",
    "predictions.select(\"Class\", \"prediction\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8282091720044809\n",
      "Test Error = 0.17179082799551915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/10 09:27:54 WARN DAGScheduler: Broadcasting large task binary with size 1279.7 KiB\n",
      "22/05/10 09:27:54 WARN DAGScheduler: Broadcasting large task binary with size 1596.3 KiB\n",
      "22/05/10 09:27:56 WARN DAGScheduler: Broadcasting large task binary with size 1241.1 KiB\n",
      "22/05/10 09:28:00 WARN DAGScheduler: Broadcasting large task binary with size 1451.5 KiB\n",
      "22/05/10 09:28:00 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/05/10 09:28:01 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/05/10 09:28:02 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "22/05/10 09:28:04 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/05/10 09:28:12 WARN DAGScheduler: Broadcasting large task binary with size 1140.5 KiB\n",
      "22/05/10 09:28:12 WARN DAGScheduler: Broadcasting large task binary with size 1411.8 KiB\n",
      "22/05/10 09:28:13 WARN DAGScheduler: Broadcasting large task binary with size 1120.3 KiB\n",
      "22/05/10 09:28:18 WARN DAGScheduler: Broadcasting large task binary with size 1323.8 KiB\n",
      "22/05/10 09:28:18 WARN DAGScheduler: Broadcasting large task binary with size 1854.6 KiB\n",
      "22/05/10 09:28:19 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/05/10 09:28:20 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/05/10 09:28:22 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/05/10 09:28:31 WARN DAGScheduler: Broadcasting large task binary with size 1137.0 KiB\n",
      "22/05/10 09:28:31 WARN DAGScheduler: Broadcasting large task binary with size 1403.4 KiB\n",
      "22/05/10 09:28:33 WARN DAGScheduler: Broadcasting large task binary with size 1094.7 KiB\n",
      "22/05/10 09:28:39 WARN DAGScheduler: Broadcasting large task binary with size 1325.5 KiB\n",
      "22/05/10 09:28:40 WARN DAGScheduler: Broadcasting large task binary with size 1852.6 KiB\n",
      "22/05/10 09:28:41 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/05/10 09:28:42 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/05/10 09:28:45 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/05/10 09:30:07 WARN DAGScheduler: Broadcasting large task binary with size 1287.7 KiB\n",
      "22/05/10 09:30:07 WARN DAGScheduler: Broadcasting large task binary with size 1609.3 KiB\n",
      "22/05/10 09:30:09 WARN DAGScheduler: Broadcasting large task binary with size 1240.8 KiB\n",
      "22/05/10 09:30:13 WARN DAGScheduler: Broadcasting large task binary with size 1464.3 KiB\n",
      "22/05/10 09:30:14 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/05/10 09:30:15 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "22/05/10 09:30:16 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "22/05/10 09:30:18 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/05/10 09:30:25 WARN DAGScheduler: Broadcasting large task binary with size 1092.4 KiB\n",
      "22/05/10 09:30:26 WARN DAGScheduler: Broadcasting large task binary with size 1349.3 KiB\n",
      "22/05/10 09:30:27 WARN DAGScheduler: Broadcasting large task binary with size 1072.9 KiB\n",
      "22/05/10 09:30:31 WARN DAGScheduler: Broadcasting large task binary with size 1336.3 KiB\n",
      "22/05/10 09:30:32 WARN DAGScheduler: Broadcasting large task binary with size 1871.9 KiB\n",
      "22/05/10 09:30:33 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/05/10 09:30:34 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/05/10 09:30:36 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/05/10 09:30:45 WARN DAGScheduler: Broadcasting large task binary with size 1100.2 KiB\n",
      "22/05/10 09:30:45 WARN DAGScheduler: Broadcasting large task binary with size 1354.7 KiB\n",
      "22/05/10 09:30:47 WARN DAGScheduler: Broadcasting large task binary with size 1064.9 KiB\n",
      "22/05/10 09:30:51 WARN DAGScheduler: Broadcasting large task binary with size 1313.7 KiB\n",
      "22/05/10 09:30:52 WARN DAGScheduler: Broadcasting large task binary with size 1824.0 KiB\n",
      "22/05/10 09:30:53 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/05/10 09:30:54 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "22/05/10 09:30:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/05/10 09:32:22 WARN DAGScheduler: Broadcasting large task binary with size 1291.8 KiB\n",
      "22/05/10 09:32:22 WARN DAGScheduler: Broadcasting large task binary with size 1603.8 KiB\n",
      "22/05/10 09:32:23 WARN DAGScheduler: Broadcasting large task binary with size 1237.1 KiB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:851\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=849'>850</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=850'>851</a>\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_items\u001b[39m.\u001b[39;49mpopleft()\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=851'>852</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/carlosv/maestria/bigdata/repo_new/big_data_malware/random_forests.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/carlosv/maestria/bigdata/repo_new/big_data_malware/random_forests.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39m# Run cross validations.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/carlosv/maestria/bigdata/repo_new/big_data_malware/random_forests.ipynb#ch0000016?line=1'>2</a>\u001b[0m \u001b[39m# rfModel = rf.fit(train)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/carlosv/maestria/bigdata/repo_new/big_data_malware/random_forests.ipynb#ch0000016?line=2'>3</a>\u001b[0m \u001b[39m# predictions = rfModel.transform(test)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/carlosv/maestria/bigdata/repo_new/big_data_malware/random_forests.ipynb#ch0000016?line=3'>4</a>\u001b[0m rfcvModel \u001b[39m=\u001b[39m rfcv\u001b[39m.\u001b[39;49mfit(train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/carlosv/maestria/bigdata/repo_new/big_data_malware/random_forests.ipynb#ch0000016?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(rfcvModel)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/carlosv/maestria/bigdata/repo_new/big_data_malware/random_forests.ipynb#ch0000016?line=6'>7</a>\u001b[0m \u001b[39m# Use test set here so we can measure the accuracy of our model on new data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/base.py?line=161'>162</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/base.py?line=162'>163</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/base.py?line=163'>164</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/tuning.py:689\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/tuning.py?line=683'>684</a>\u001b[0m train \u001b[39m=\u001b[39m datasets[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcache()\n\u001b[1;32m    <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/tuning.py?line=685'>686</a>\u001b[0m tasks \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[1;32m    <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/tuning.py?line=686'>687</a>\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/tuning.py?line=687'>688</a>\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam))\n\u001b[0;32m--> <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/tuning.py?line=688'>689</a>\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/tuning.py?line=689'>690</a>\u001b[0m     metrics[j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (metric \u001b[39m/\u001b[39m nFolds)\n\u001b[1;32m    <a href='file:///home/carlosv/.local/lib/python3.8/site-packages/pyspark/ml/tuning.py?line=690'>691</a>\u001b[0m     \u001b[39mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=853'>854</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=854'>855</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=855'>856</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=856'>857</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/multiprocessing/pool.py?line=857'>858</a>\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/threading.py?line=299'>300</a>\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/threading.py?line=300'>301</a>\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/lib/python3.8/threading.py?line=301'>302</a>\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/threading.py?line=302'>303</a>\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/threading.py?line=303'>304</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/10 09:32:27 WARN DAGScheduler: Broadcasting large task binary with size 1467.1 KiB\n",
      "22/05/10 09:32:28 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/05/10 09:32:29 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "22/05/10 09:32:29 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "22/05/10 09:32:31 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/05/10 09:32:38 WARN DAGScheduler: Broadcasting large task binary with size 1123.1 KiB\n",
      "22/05/10 09:32:38 WARN DAGScheduler: Broadcasting large task binary with size 1401.7 KiB\n",
      "22/05/10 09:32:39 WARN DAGScheduler: Broadcasting large task binary with size 1122.0 KiB\n",
      "22/05/10 09:32:42 WARN DAGScheduler: Broadcasting large task binary with size 1346.6 KiB\n",
      "22/05/10 09:32:43 WARN DAGScheduler: Broadcasting large task binary with size 1887.2 KiB\n",
      "22/05/10 09:32:43 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "22/05/10 09:32:44 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/05/10 09:32:45 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/05/10 09:32:52 WARN DAGScheduler: Broadcasting large task binary with size 1090.0 KiB\n",
      "22/05/10 09:32:53 WARN DAGScheduler: Broadcasting large task binary with size 1345.3 KiB\n",
      "22/05/10 09:32:54 WARN DAGScheduler: Broadcasting large task binary with size 1056.7 KiB\n",
      "22/05/10 09:32:58 WARN DAGScheduler: Broadcasting large task binary with size 1330.7 KiB\n",
      "22/05/10 09:32:59 WARN DAGScheduler: Broadcasting large task binary with size 1858.9 KiB\n",
      "22/05/10 09:32:59 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/05/10 09:33:00 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "22/05/10 09:33:02 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n"
     ]
    }
   ],
   "source": [
    "# Run cross validations.\n",
    "# rfModel = rf.fit(train)\n",
    "# predictions = rfModel.transform(test)\n",
    "rfcvModel = rfcv.fit(train)\n",
    "print(rfcvModel)\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = rfcvModel.transform(test)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
