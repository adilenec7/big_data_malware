{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza y extracción de caracteristicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de extraer las características, es necesario la limpieza de los archivos bytecode, debido a que estos tienen la dirección de los bytecode (lo que está señalado en la imagen, también conocido como offset), ya que este es el mismo para todos los archivos. Así como en los métodos de extracción de características, aquí se usó como base el método propuesto por Rohan-Paul-Al (Pendiente:Agregar link a su notebook), sin embargo, se le realizaron algunas modificaciones para poder correrlo en paralelo la librería MultiProcessing y así corriera de manera más rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('../big_data/training_set/byteFiles')\n",
    "\n",
    "def process(file):\n",
    "    file=file.split('.')[0]\n",
    "    text_file = open('byteFiles/'+file+\".txt\", 'w+')\n",
    "    with open('byteFiles/'+file+\".bytes\",\"r\") as fp:\n",
    "        lines=\"\"\n",
    "        for line in fp:\n",
    "            a=line.rstrip().split(\" \")[1:]\n",
    "            b=' '.join(a)\n",
    "            b=b+\"\\n\"\n",
    "            text_file.write(b)\n",
    "        fp.close()\n",
    "        os.remove('byteFiles/'+file+\".bytes\")\n",
    "    text_file.close()\n",
    "\n",
    "#Corriendolo en paralelo\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "files = os.listdir('../big_data/training_set/byteFiles')\n",
    "pool = Pool(10)\n",
    "results = pool.map(process, files)\n",
    "pool.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de caracteristicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción de unigramas: Para extraer las características de los archivos bytecode se utilizó el enfoque de la extracción de unigramas. Se intentaron diversas formas (tales como utilizar el método CountVectorizer que viene integrado en PySpark ML), sin embargo, el tamaño de los archivos eran muy grandes y había una gran cantidad de ellos, lo cual hacía que el proceso fuera muy lento (o causaba errores).\n",
    "Finalmente, la metodología que resultó exitosa fue inspirada en la solución propuesta por Rohan-Paul-Al, pero se modificó ligeramente para adaptarlo a la librería MultiProcessing de Python, el cual, por medio de la paralelización del proceso acelera considerablemente la ejecución del código, como se muestra a continuación: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "#Definiendo el proceso con el que se extraerán las caracteristicas de cada archivo\n",
    "def process(file):\n",
    "    filenames2=[]\n",
    "    k=0\n",
    "    feature_matrix = np.zeros((len(files),257),dtype=int)\n",
    "    nombre_archivo = str(file)[:-4] +'.csv'\n",
    "    byte_feature_file=open(nombre_archivo,'w+')\n",
    "    byte_feature_file.write(\"ID,0,1,2,3,4,5,6,7,8,9,0a,0b,0c,0d,0e,0f,10,11,12,13,14,15,16,17,18,19,1a,1b,1c,1d,1e,1f,20,21,22,23,24,25,26,27,28,29,2a,2b,2c,2d,2e,2f,30,31,32,33,34,35,36,37,38,39,3a,3b,3c,3d,3e,3f,40,41,42,43,44,45,46,47,48,49,4a,4b,4c,4d,4e,4f,50,51,52,53,54,55,56,57,58,59,5a,5b,5c,5d,5e,5f,60,61,62,63,64,65,66,67,68,69,6a,6b,6c,6d,6e,6f,70,71,72,73,74,75,76,77,78,79,7a,7b,7c,7d,7e,7f,80,81,82,83,84,85,86,87,88,89,8a,8b,8c,8d,8e,8f,90,91,92,93,94,95,96,97,98,99,9a,9b,9c,9d,9e,9f,a0,a1,a2,a3,a4,a5,a6,a7,a8,a9,aa,ab,ac,ad,ae,af,b0,b1,b2,b3,b4,b5,b6,b7,b8,b9,ba,bb,bc,bd,be,bf,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,ca,cb,cc,cd,ce,cf,d0,d1,d2,d3,d4,d5,d6,d7,d8,d9,da,db,dc,dd,de,df,e0,e1,e2,e3,e4,e5,e6,e7,e8,e9,ea,eb,ec,ed,ee,ef,f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,fa,fb,fc,fd,fe,ff,??\")\n",
    "\n",
    "    byte_feature_file.write(\"\\n\")\n",
    "    filenames2.append(file)\n",
    "    byte_feature_file.write(file+\",\")\n",
    "    #Abre el archivo\n",
    "    with open('../big_data/training_set/byteFiles/'+file,\"r\") as byte_flie:\n",
    "        for lines in byte_flie:\n",
    "            line=lines.rstrip().split(\" \")\n",
    "            for hex_code in line:\n",
    "                if hex_code=='??':\n",
    "                    feature_matrix[k][256]+=1\n",
    "                else:\n",
    "                    feature_matrix[k][int(hex_code,16)]+=1\n",
    "    for i, row in enumerate(feature_matrix[k]):\n",
    "        if i!=len(feature_matrix[k])-1:\n",
    "            byte_feature_file.write(str(row)+\",\")\n",
    "        else:\n",
    "            byte_feature_file.write(str(row))\n",
    "    byte_feature_file.write(\"\\n\")\n",
    "\n",
    "#Corriendo el proceso en paralelo\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "files = os.listdir('../big_data/training_set/byteFiles')\n",
    "pool = Pool(10)\n",
    "results = pool.map(process, files)\n",
    "pool.close()\n",
    "\n",
    "\n",
    "#Ahora concatenaremos todos los archivos CSV en forma paralela\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import pandas as pd\n",
    "from glob import glob \n",
    "\n",
    "files = glob(\"*.csv\")\n",
    "\n",
    "def read_file(file):\n",
    "    return pd.read_csv(file)\n",
    "#Corriendo el proceso en paralelo y obteniendo el .csv\n",
    "with ThreadPoolExecutor(4) as pool:\n",
    "    df = pd.concat(pool.map(read_file, files))\n",
    "df.to_csv('unigrams_completo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando tamaño de los archivos como caracteristicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción del peso de archivo: Otra característica que se contempló fue la extracción del peso del archivo, este extracción de características no es costosa (computacionalmente hablando) y se realizó utilizando como base el método propuesto por Rohan-Paul-Al, adaptándolo a PySpark:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Codigo modificado de https://www.kaggle.com/code/paulrohan2020/microsoft-malware-detection-log-loss-of-0-0070\n",
    "# Todos los créditos van a Rohan-Paul-Al\n",
    "\n",
    "files=os.listdir('training_set/byteFiles')\n",
    "\n",
    "filenames=label_data['Id'].tolist()\n",
    "class_y=label_data['Class'].tolist()\n",
    "class_bytes=[]\n",
    "sizebytes=[]\n",
    "fnames=[]\n",
    "for file in files:\n",
    "    print('Trying for ', file)\n",
    "    statinfo=os.stat('training_set/byteFiles/'+file)\n",
    "    # split the file name at '.' and take the first part of it i.e the file name\n",
    "    file=file.split('.')[0]\n",
    "    print(file)\n",
    "    if any(file == filename for filename in filenames):\n",
    "        i=filenames.index(file)\n",
    "        class_bytes.append(class_y[i])\n",
    "        # converting into Mb's\n",
    "        sizebytes.append(statinfo.st_size/(1024.0*1024.0))\n",
    "        fnames.append(file)\n",
    "\n",
    "#Pasandolo a dataframe de spark\n",
    "data_size_byte=ps.DataFrame({'ID':fnames,'size':sizebytes,'Class':class_bytes})\n",
    "print (data_size_byte.head())\n",
    "\n",
    "##Convertiendolo a CSV\n",
    "data_size_byte.to_spark().toPandas().to_csv('file_size_data.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
